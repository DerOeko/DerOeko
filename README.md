I build systems that autonomously find failure modes in Large Language Models using Reinforcement Learning.

Currently, I am working at Kachman Lab on Slingshot, an automated jailbreaking framework targeting ICML. My work focuses on the intersection of theoretical alignment research and high-performance engineering.

Current focus: 
* Reinforcement Learning: Implementing verifiable reward frameworks (using GRPO and Verifiers) to train adversarial agents.
* Inference Optimization: Designing asynchronous generation pipelines using asyncio and vLLM to maximize throughput across multi-GPU environments.
* Evaluation: Curating adversarial datasets and benchmarking model robustness using AgentDojo and custom environments.

Selected work:
* Slingshot (Current): End-to-end RL training for automated jailbreaking.
* Prime Intellect: Developed RL environments for decentralized training (e.g., literary analysis).
* RoboLearn: Build bayesian modeling of depression (controllability) and data analysis pipeline for computational psychiatry (Donders Institute/RadboudUMC).

Interests: Adversarial Robustness • Model Evals • Alignment Faking • Model Organisms • Mechanistic Interpretability • Steganography • Jailbreaking • Multi-Agent Systems.

Wanna talk? Feel free to [book a 1-on-1](https://calendar.app.google/3c9J24Pbcb1F558c9). Always happy to chat. Or the old fashioned way: samuelgerrit.nellessen{at}gmail.com.
